# Ollama本地大模型快速开始指南

## 为什么选择Ollama?

✅ **完全免费** - 无需购买API密钥,无调用次数限制  
✅ **数据安全** - 所有数据在本地处理,不会上传到云端  
✅ **响应快速** - 本地运行,不受网络限制  
✅ **易于使用** - 一键安装,简单配置即可使用  
✅ **模型丰富** - 支持Llama、Qwen、Mistral等多种开源模型  

## 5分钟快速上手

### 第一步: 安装Ollama

#### Windows用户
1. 访问 https://ollama.ai/download/windows
2. 下载并运行安装程序
3. 安装完成后,Ollama会自动在后台运行

#### macOS用户
1. 访问 https://ollama.ai/download/mac
2. 下载.dmg文件
3. 将Ollama拖到应用程序文件夹
4. 打开Ollama应用

#### Linux用户
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 第二步: 下载AI模型

打开终端(Windows用户打开CMD或PowerShell),运行以下命令:

```bash
# 推荐: 下载通义千问2模型(中文能力强)
ollama pull qwen2

# 或者下载Llama 3模型(英文能力强)
ollama pull llama3
```

**注意**: 首次下载需要几分钟到十几分钟,模型文件约4-7GB

### 第三步: 验证安装

```bash
# 查看已下载的模型
ollama list

# 测试模型(可选)
ollama run qwen2
```

如果看到模型列表,说明安装成功!

### 第四步: 在系统中配置

1. 登录库存管理系统(使用管理员账号)
2. 点击左侧菜单 **系统管理** → **系统设置**
3. 切换到 **AI配置** 标签页
4. 配置如下:
   - **AI服务商**: 选择 "Ollama本地模型"
   - **模型**: 选择 "qwen2" (或您下载的其他模型)
   - **API密钥**: 留空(无需填写)
   - **API地址**: 留空(使用默认地址)
5. 点击 **保存配置**

### 第五步: 开始使用

1. 点击左侧菜单 **查询报表** → **AI智能分析**
2. 输入问题或点击快速提问
3. 点击 **开始分析**
4. 等待AI给出分析结果

## 推荐模型对比

| 模型名称 | 参数量 | 中文能力 | 英文能力 | 速度 | 内存需求 | 推荐场景 |
|---------|--------|---------|---------|------|---------|---------|
| **qwen2** | 7B | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 快 | 8GB | 中文为主,推荐 |
| **llama3** | 8B | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 快 | 8GB | 英文为主 |
| **mistral** | 7B | ⭐⭐ | ⭐⭐⭐⭐⭐ | 快 | 8GB | 英文专业场景 |
| **phi** | 3B | ⭐⭐⭐ | ⭐⭐⭐⭐ | 很快 | 4GB | 低配置电脑 |
| **qwen** | 7B | ⭐⭐⭐⭐ | ⭐⭐⭐ | 快 | 8GB | qwen2的前代 |

**推荐**: 
- 中文用户首选 **qwen2**
- 英文用户首选 **llama3**
- 低配置电脑选 **phi**

## 常见问题

### Q: 下载模型很慢怎么办?
A: 模型文件较大(4-7GB),首次下载需要时间。可以:
- 使用稳定的网络环境
- 耐心等待,下载完成后就可以离线使用了

### Q: 提示"连接失败"怎么办?
A: 检查Ollama服务是否运行:
```bash
# 查看Ollama状态
ollama list

# 如果没有响应,手动启动服务
ollama serve
```

### Q: 我的电脑配置够吗?
A: 最低要求:
- CPU: 现代多核处理器
- 内存: 8GB以上(推荐16GB)
- 硬盘: 10GB可用空间
- 系统: Windows 10+, macOS 11+, Linux

### Q: 可以同时使用多个模型吗?
A: 可以!下载多个模型后,在系统设置中切换即可:
```bash
ollama pull qwen2
ollama pull llama3
ollama pull phi
```

### Q: 如何删除不用的模型?
A: 使用以下命令:
```bash
ollama rm 模型名
```

### Q: Ollama占用太多内存怎么办?
A: Ollama会自动管理内存,不使用时会释放。如需手动释放:
- Windows: 在任务管理器中结束Ollama进程
- macOS/Linux: `killall ollama`

### Q: 分析速度慢怎么办?
A: 
- 首次运行会加载模型,需要几秒钟
- 后续运行会更快
- 可以尝试更小的模型(如phi)
- 确保电脑没有运行其他大型程序

## 高级技巧

### 1. 使用自定义模型
如果您下载了其他模型,在系统设置中选择"custom",然后在模型名称中填写实际的模型名。

### 2. 修改API地址
如果Ollama运行在其他端口或其他机器上:
```
http://192.168.1.100:11434/api/chat
```

### 3. 命令行测试
在配置系统前,可以先在命令行测试:
```bash
ollama run qwen2
>>> 你好,请介绍一下自己
```

### 4. 查看模型详情
```bash
ollama show qwen2
```

## 更多资源

- Ollama官网: https://ollama.ai/
- 模型库: https://ollama.ai/library
- GitHub: https://github.com/ollama/ollama
- 中文社区: 搜索"Ollama中文"

## 技术支持

如遇到问题:
1. 查看本文档的"常见问题"部分
2. 查看Ollama官方文档
3. 联系系统管理员

---

**开始享受免费、安全、快速的AI智能分析吧!** 🚀
